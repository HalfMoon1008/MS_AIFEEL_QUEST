{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "982dee4d",
   "metadata": {},
   "source": [
    "# 18-1. Project: 멋진 챗봇 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6822b10f",
   "metadata": {},
   "source": [
    "## 라이브러리 버전을 확인해 봅니다\n",
    "\n",
    "---\n",
    "\n",
    "사용할 라이브러리 버전을 둘러봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "083e2b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.21.4\n",
      "1.3.3\n",
      "2.6.0\n",
      "3.6.5\n",
      "4.1.2\n"
     ]
    }
   ],
   "source": [
    "import numpy \n",
    "import pandas \n",
    "import tensorflow \n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "print(numpy.__version__)\n",
    "print(pandas.__version__)\n",
    "print(tensorflow.__version__)\n",
    "print(nltk.__version__)\n",
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c1fc00",
   "metadata": {},
   "source": [
    "## Step 1. 데이터 다운로드\n",
    "\n",
    "---\n",
    "\n",
    "`ChatbotData .csv`를 `pandas` 라이브러리로 읽어서 데이터의 질문과 답변을 각각 `questions`, `answers` 변수에 나눠서 저장하기  \n",
    "- [songys/Chatbot_data](https://github.com/songys/Chatbot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c1bd0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용할 모듈들 선언\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from konlpy.tag import Mecab\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fe5df5",
   "metadata": {},
   "source": [
    "*파일 경로 선언*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "059590da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aiffel/aiffel/transformer_chatbot/data/ChatbotData.csv\n"
     ]
    }
   ],
   "source": [
    "file_dir = '/aiffel/aiffel/transformer_chatbot/data'\n",
    "file_name = 'ChatbotData.csv'\n",
    "file_path = '/'.join([file_dir, file_name])\n",
    "\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b42eff",
   "metadata": {},
   "source": [
    "*CSV 읽고 데이터 확인하기*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ae04703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(file_path)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2acf361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 질문 저장\n",
    "questions = [q for q in dataset['Q']]\n",
    "# 데이터 답변 저장\n",
    "answers = [a for a in dataset['A']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3d50de",
   "metadata": {},
   "source": [
    "*저장된 질문, 답변 확인*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "145a7260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['12시 땡!', '1지망 학교 떨어졌어', '3박4일 놀러가고 싶다', '3박4일 정도 놀러가고 싶다', 'PPL 심하네']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c65a1e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11823"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f1777ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['하루가 또 가네요.', '위로해 드립니다.', '여행은 언제나 좋죠.', '여행은 언제나 좋죠.', '눈살이 찌푸려지죠.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "367667d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11823"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d04a87",
   "metadata": {},
   "source": [
    "## Step 2. 데이터 정제\n",
    "\n",
    "---\n",
    "\n",
    "아래 조건을 만족하는 `preprocess_sentence()` 함수 구현하기  \n",
    "1. 영문자의 경우, **모두 소문자로 변환**\n",
    "2. 영문자와 한글, 숫자, 그리고 주요 특수문자를 제외하곤 **정규식을 활용하여 모두 제거**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25eba5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    # 1. 영문자의 경우, 모두 소문자로 변환\n",
    "    sentence = sentence.lower()\n",
    "    # 2. 정규식을 활용하여 조건에 해당하는 것들을 제거\n",
    "    sentence = re.sub(r'([^a-zA-Zㄱ-ㅎ가-힣!?])', \" \", sentence)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fb032e",
   "metadata": {},
   "source": [
    "## Step 3. 데이터 토큰화\n",
    "\n",
    "---\n",
    "\n",
    "토큰화 &rarr; KoNLPy의 `mecab` 클래스 사용  \n",
    "아래 조건을 만족하는 `build_corpus()` 함수 구현하기  \n",
    "1. **소스 문장 데이터, 타켓 문장 데이터** 입력으로 받기\n",
    "2. 데이터를 `preprocess_sentece()`로 **정제하고, 토큰화**\n",
    "3. 토큰화 &rarr; **전달 받은 토크나이즈 함수 사용** (`mecab.morphs`)\n",
    "4. 토큰의 개수가 일정 길이 이상인 문장은 **데이터에서 제외**\n",
    "5. **중복되는 문장은 데이터에서 제외**\n",
    "    - `소스`는 소스대로, `타겟`은 타겟대로 검사, 중복 쌍이 흐트러지지 않도록 주의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7e01197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수구현; 소스 문장, 타겟 문장 데이터들 입력으로 받기\n",
    "## 소스 --> 질문(question), 타켓 --> 답변(answer)\n",
    "def build_corpus(src, tgt):\n",
    "    # 토큰 길이 제한 값 (암의설정)\n",
    "    MAX_LEN = 50\n",
    "    \n",
    "    # 데이터 정제\n",
    "    src = preprocess_sentence(src)\n",
    "    tgt = preprocess_sentence(tgt)\n",
    "    # 토큰화\n",
    "    mecab = Mecab() # Mecab 인스턴스 생성\n",
    "    ## 토큰의 개수가 일정 길이(MAX_LEN) 이상인 문장은 제외\n",
    "    if MAX_LEN < len(mecab.morphs(src)):\n",
    "        src_corpus = mecab.morphs(src)\n",
    "        # 중복되는 문장은 데이터에서 제외\n",
    "        src_corpus = list(set(src_corpus))\n",
    "    if MAX_LEN < len(mecab.morphs(tgt)):\n",
    "        tgt_corpus = mecab.morphs(tgt)\n",
    "        # 중복되는 문장은 데이터에서 제외\n",
    "        tgt_corpus = list(set(tgt_corpus))\n",
    "    \n",
    "    # 각 데이터별로 검사 후 중복 쌍이 흐트러지지 않도록 List로 감싸서 return\n",
    "    return [src_corpus, tgt_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d093a1",
   "metadata": {},
   "source": [
    "## Step 4. Augmentation\n",
    "\n",
    "---\n",
    "\n",
    "주어진 데이터: **약 1만 개** (데이터가 적으므로 **Lexical Substitution 적용**)  \n",
    "아래 링크 참고해 **한국어로 사전 훈련된 Embedding 모델 다운로드**.  \n",
    "`Korean (w)`를 사이트에서 찾아 다운로드하고, `ko.bin` 파일을 얻기\n",
    "\n",
    "- [Kyubyong/wordvectors](https://github.com/Kyubyong/wordvectors)\n",
    "\n",
    "다운로드한 모델 활용해 **데이터 Augmentation** (앞서 정의한 `lexical_sub()` 함수 참고)  \n",
    "1. `que_corpus`: Augmentation 된 말뭉치\n",
    "2. `ans_corpus`: 원본 말뭉치\n",
    "\n",
    "---\n",
    "\n",
    "3. `que_corpus`: 원본 말뭉치\n",
    "4. `ans_corpus`: Augmentation 된 말뭉치\n",
    "\n",
    "> `1:2`, `3:4`가 병렬을 이루도록하여 **전체 데이터가 원래의 3배 가량으로 늘어나도록 조치**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3132986e",
   "metadata": {},
   "source": [
    "- 위 링크에서 필요한 파일들 다운로드 완료 (local)\n",
    "- 이후 LMS 업로드 완료 (`*.bin`, `*.tsv`)\n",
    "- 파일 경로 (디렉토리 까지만 표시함)\n",
    "    > `/aiffel/aiffel/transformer_chatbot/data/Kyubyong_wordvectors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e0987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "#### 사전 학습된 모델을 가지고 텍스트를 Augmentation (일부 단어 대체)하는 함수 선언\n",
    "\n",
    "# 모델 경로 및 토크나이저 선택\n",
    "pre_trained_model_path = '/aiffel/aiffel/transformer_chatbot/data/Kyubyong_wordvectors/ko.bin'\n",
    "pre_trained_tokenizer = AutoTokenizer.from_pretrained(\"kykim/bert-kor-base\")\n",
    "\n",
    "# 모델 로드\n",
    "pre_trained_model = AutoModelForMaskedLM.from_pretrained(pre_trained_model_path)\n",
    "\n",
    "# lexical_sub() 함수 정의\n",
    "def lexical_substitution(text, model, tokenizer):\n",
    "    # 입력 텍스트를 토큰화\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    # 무작위로 대체할 토큰을 선택\n",
    "    token_to_replace = random.choice(tokens)\n",
    "    \n",
    "    # 선택한 토큰을 '[MASK]'로 대체\n",
    "    tokens[tokens.index(token_to_replace)] = '[MASK]'\n",
    "    \n",
    "    # 토큰을 모델 입력 형식으로 변환\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    \n",
    "    # 모델에 입력을 전달하여 대체 단어 예측\n",
    "    with tf.device('/GPU:0'):\n",
    "        input_tensor = tf.constant([input_ids])\n",
    "        outputs = model(input_tensor)\n",
    "        predictions = outputs.logits[0]\n",
    "    \n",
    "    # 각 [MASK] 위치에서 가장 유사한 단어로 대체\n",
    "    for i, token_id in enumerate(input_ids):\n",
    "        if token_id == tokenizer.mask_token_id:\n",
    "            predicted_index = np.argmax(predictions[i])\n",
    "            predicted_word = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "            tokens[i] = predicted_word\n",
    "    \n",
    "    # 대체된 토큰을 문장으로 복원\n",
    "    augmented_text = tokenizer.convert_tokens_to_string(tokens)\n",
    "    \n",
    "    return augmented_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9686149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 questions를 돌면서 Augmentation 수행해 text 내용 일부 변경하기\n",
    "augmented_questions = []\n",
    "\n",
    "for sentence in questions:\n",
    "    augmented_questions.append()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862a5a53",
   "metadata": {},
   "source": [
    "## Step 5. 데이터 벡터화\n",
    "\n",
    "---\n",
    "\n",
    "`ans_corpus`: 타겟 데이터 (`<start>`, `<end>` 토큰을 추가해주고 벡터화 진행하기)\n",
    "\n",
    "```python\n",
    "sample_data = [\"12\", \"시\", \"땡\", \"!\"]\n",
    "\n",
    "print([\"<start>\"] + sample_data + [\"<end>\"])\n",
    "```\n",
    "```shell\n",
    "['<start>', '12', '시', '땡', '!', '<end>']\n",
    "```\n",
    "\n",
    "1. 위 소스 참고하여 타겟 데이터 전체에 토큰 추가 (`<start>`, `<end>`)\n",
    "2. 특수 토큰이 더해진 `ans_corpus`에 `que_corpus`를 결합하여 **전체 데이터에 대한 단어 사전을 구축하고 벡터화**하여 `enc_train`, `dec_train` 얻기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9be1eac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5371b169",
   "metadata": {},
   "source": [
    "## Step 6. 훈련하기\n",
    "\n",
    "---\n",
    "\n",
    "앞서 훈련한 모델 `Transformer`를 그대로 사용하기  \n",
    "대신 데이터 크기 작으니 하이퍼파라미터를 튜닝하여 과적합 피하기  \n",
    "모델 훈련하고 아래 예문에 대한 답변을 생성하기\n",
    "\n",
    "```shell\n",
    "# 예문\n",
    "1. 지루하다, 놀러가고 싶어.\n",
    "2. 오늘 일찍 일어났더니 피곤하다.\n",
    "3. 간만에 여자친구랑 데이트 하기로 했어.\n",
    "4. 집에 있는다는 소리야.\n",
    "\n",
    "---\n",
    "\n",
    "# 제출\n",
    "\n",
    "Translations\n",
    "> 1. 잠깐 쉬 어도 돼요 . <end>\n",
    "> 2. 맛난 거 드세요 . <end>\n",
    "> 3. 떨리 겠 죠 . <end>\n",
    "> 4. 좋 아 하 면 그럴 수 있 어요 . <end>\n",
    "\n",
    "Hyperparameters\n",
    "> n_layers: 1\n",
    "> d_model: 368\n",
    "> n_heads: 8\n",
    "> d_ff: 1024\n",
    "> dropout: 0.2\n",
    "\n",
    "Training Parameters\n",
    "> Warmup Steps: 1000\n",
    "> Batch Size: 64\n",
    "> Epoch At: 10\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774fc558",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96a48826",
   "metadata": {},
   "source": [
    "## Step 7. 성능 측정하기\n",
    "\n",
    "---\n",
    "\n",
    "1. 주어진 질문에 적절한 답변을 하는지 확인\n",
    "2. BLEU Score를 계산하는 `calculate_bleu()` 함수도 적용해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc5a3d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61304593",
   "metadata": {},
   "source": [
    "### 루브릭\n",
    "#### 아래의 기준을 바탕으로 프로젝트를 평가합니다.\n",
    "\n",
    "|**평가문항**|**상세기준**|\n",
    "|---|---|\n",
    "|1. 챗봇 훈련데이터 전처리 과정이 체계적으로 진행되었는가?|챗봇 훈련데이터를 위한 전처리와 augmentation이 적절히 수행되어 3만개 가량의 훈련데이터셋이 구축되었다.|\n",
    "|2. transformer 모델을 활용한 챗봇 모델이 과적합을 피해 안정적으로 훈련되었는가?|과적합을 피할 수 있는 하이퍼파라미터 셋이 적절히 제시되었다.|\n",
    "|3. 챗봇이 사용자의 질문에 그럴듯한 형태로 답하는 사례가 있는가?|주어진 예문을 포함하여 챗봇에 던진 질문에 적절히 답하는 사례가 제출되었다.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55dd44f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
