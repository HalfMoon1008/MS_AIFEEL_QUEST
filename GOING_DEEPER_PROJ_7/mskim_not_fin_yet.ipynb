{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "998d9362",
   "metadata": {},
   "source": [
    "# **24. HuggingFace 커스텀 프로젝트 만들기 [프로젝트]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7637248",
   "metadata": {},
   "source": [
    "## **24-1. 프로젝트 : 커스텀 프로젝트 직접 만들기**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66c08c0",
   "metadata": {},
   "source": [
    "실습 코드에서 수행해 본 내용을 토대로, 이번에는 한국어 데이터셋에 도전해보겠습니다.\n",
    "\n",
    "앞서 본 GLUE benchmark의 한국어 버전 [KLUE benchmark](https://klue-benchmark.com/)를 들어보신 적 있나요?\n",
    "\n",
    "GLUE와 마찬가지로 한국어 자연어처리에 대한 이해도를 높이기 위해 만들어진 데이터셋 benchmark입니다. 총 8가지의 데이터셋이 있습니다. 다만 이번 시간에 진행할 프로젝트는 KLUE의 dataset을 활용하는 것이 아닌, model(klue/ber-base)를 활용하여 NSMC(Naver Sentiment Movie Corpus) task를 도전해보겠습니다.\n",
    "\n",
    "모델과 데이터에 관한 정보는 링크를 참조해주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e110624",
   "metadata": {},
   "source": [
    "- [KLUE/Bert-base](https://huggingface.co/klue/bert-base)\n",
    "- [NSMC](https://github.com/e9t/nsmc)\n",
    "\n",
    "<br>\n",
    "준비가 되셨다면 아래와 같은 순서로 진행해주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b382e80c",
   "metadata": {},
   "source": [
    "### **라이브러리 버전을 확인해 봅니다.**\n",
    "\n",
    "---\n",
    "\n",
    "사용할 라이브러리 버전을 둘러봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd34db57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow\n",
    "# import numpy\n",
    "# import transformers\n",
    "# import datasets\n",
    "\n",
    "# print(tensorflow.__version__)\n",
    "# print(numpy.__version__)\n",
    "# print(transformers.__version__)\n",
    "# print(datasets.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c42a95",
   "metadata": {},
   "source": [
    "### **STEP 1. NSMC 데이터 분석 및 Huggingface dataset 구성**\n",
    "\n",
    "---\n",
    "\n",
    "- 데이터셋은 깃허브에서 다운받거나, [Huggingface datasets](https://huggingface.co/datasets)에서 가져올 수 있습니다. 앞에서 배운 방법들을 활용해봅시다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a74744ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용할 모듈 설정\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoTokenizer, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "579b47bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset nsmc (/aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39ca75fd0dab43cf99def54694f5bc22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'document', 'label'],\n",
      "        num_rows: 150000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'document', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# nsmc 데이터를 'load_dataset'으로 불러온 다음 Dataset 구조 출력\n",
    "nsmc = load_dataset('nsmc')\n",
    "print(nsmc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e4234d",
   "metadata": {},
   "source": [
    "`train`, `test`를 각각 Key로 가지는 Dataset을 확인하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "750b9369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train length: 150000\n",
      "===========\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': '9976970', 'document': '아 더빙.. 진짜 짜증나네요 목소리', 'label': 0},\n",
       " {'id': '3819312',\n",
       "  'document': '흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나',\n",
       "  'label': 1},\n",
       " {'id': '10265843', 'document': '너무재밓었다그래서보는것을추천한다', 'label': 0},\n",
       " {'id': '9045019', 'document': '교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정', 'label': 0},\n",
       " {'id': '6483659',\n",
       "  'document': '사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다',\n",
       "  'label': 1}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 크기 확인과 5개 데이터 확인 (train)\n",
    "print(f\"train length: {len(nsmc['train']['id'])}\")\n",
    "print(\"===========\")\n",
    "# 5개의 데이터만 확인해보기\n",
    "cols = [key for key in nsmc['train']]\n",
    "cols[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950ea7f6",
   "metadata": {},
   "source": [
    "### **STEP 2. klue/bert-base model 및 tokenizer 불러오기**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4b6732d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# 모델을 변경하기 편하게 변수로 설정\n",
    "model_name = \"klue/bert-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f561a1",
   "metadata": {},
   "source": [
    "### **STEP 3. 위에서 불러온 tokenizer으로 데이터셋을 전처리하고, model 학습 진행해 보기**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb5e5409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 전처리 함수 선언\n",
    "def preprocess_text(text):\n",
    "    MAX_LEN = 60\n",
    "    \n",
    "    return tokenizer(\n",
    "        text, \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        max_length=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3985017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 데이터셋을 가지고 8:2로 분리 (train, val)\n",
    "train_dataset, val_dataset = train_test_split(\n",
    "    nsmc['train'], \n",
    "    test_size = 0.2, \n",
    "    random_state = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb69abdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 데이터셋\n",
    "test_dataset = nsmc['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "686f9697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 전처리\n",
    "train_dataset['document'] = [preprocess_text(sentence) for sentence in train_dataset['document']]\n",
    "val_dataset['document'] = [preprocess_text(sentence) for sentence in val_dataset['document']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de7f611b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 인자 설정\n",
    "training_args = TrainingArguments(\n",
    "    # 사전 학습된 모델의 batch size\n",
    "    per_device_train_batch_size=512,\n",
    "    per_device_eval_batch_size=16,\n",
    "    output_dir=os.getenv('HOME')+'/aiffel/transformers',\n",
    "    num_train_epochs=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f40f3dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝~\n"
     ]
    }
   ],
   "source": [
    "# 훈련 설정\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,  # 검증 데이터셋을 설정합니다.\n",
    ")\n",
    "\n",
    "print(\"슝~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92b21093",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 3\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 512\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 512\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_904/2846616296.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 훈련 실행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1288\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1290\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m                 \u001b[0;31m# Skip past any already trained steps if resuming training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "# 훈련 실행\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68a78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset['document'][2]\n",
    "# val_dataset['label'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bb0b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터로 모델 평가\n",
    "results = trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8ffb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 결과 출력\n",
    "print(\"Results:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78e3247",
   "metadata": {},
   "source": [
    "### **STEP 4. Fine-tuning을 통하여 모델 성능(accuarcy) 향상시키기**\n",
    "\n",
    "---\n",
    "\n",
    "- 데이터 전처리, `TrainingArguments` 등을 조정하여 모델의 정확도를 90% 이상으로 끌어올려봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dba6952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d33280a8",
   "metadata": {},
   "source": [
    "### **STEP 5. Bucketing을 적용하여 학습시키고, STEP 4의 결과와의 비교**\n",
    "- 아래 링크를 바탕으로 bucketing과 dynamic padding이 무엇인지 알아보고, 이들을 적용하여 model을 학습시킵니다.\n",
    "<br><br>\n",
    "    - [Data Collator](https://huggingface.co/docs/transformers/v4.30.0/en/main_classes/data_collator)\n",
    "    - [Trainer.TrainingArguments 의 `group_by_length`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)\n",
    "<br><br>\n",
    "- STEP 4에 학습한 결과와 bucketing을 적용하여 학습시킨 결과를 비교해보고, 모델 성능 향상과 훈련 시간 두 가지 측면에서 각각 어떤 이점이 있는지 비교해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a9b244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6519b257",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e462ae9",
   "metadata": {},
   "source": [
    "## 개인 회고\n",
    "\n",
    "1. 이전에 배웠던 것들이 엄청 다양해서 머릿속에서 뒤죽박죽 된 적이 많았다.\n",
    "    - BERT 때도 그랬고 ㅠ\n",
    "2. 지금 수준에선 좋아하거나 친숙한 형태의 구성을 모듈화 시켜놨다가 자꾸 써보는 식으로 전체 flow를 접근해야할거 같다.\n",
    "    - DLthon에서 내가 담당한 부분과 가까워서 따라가다가 더 꼬여버린 부분도 있었다(...)\n",
    "3. 어찌저찌 복구하고는 있는데 Train()에서 왜 index 에러가 나는걸까..\n",
    "    - 17:49 기준 전처리부터 이상하게 접근해버린걸 이해는 했는데 수업시간내에 완성은 안될거 같고..\n",
    "4. 초기에 metric접근부터 뭔가 이상하게 단계를 밟아나간거 같다 -_-.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d5f8f7",
   "metadata": {},
   "source": [
    "#### **루브릭**\n",
    "아래의 기준을 바탕으로 프로젝트를 평가합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b93144f",
   "metadata": {},
   "source": [
    "|평가문항|상세기준|\n",
    "|--|--|\n",
    "|1. 모델과 데이터를 정상적으로 불러오고, 작동하는 것을 확인하였다.|klue/bert-base를 NSMC 데이터셋으로 fine-tuning 하여, 모델이 정상적으로 작동하는 것을 확인하였다.|\n",
    "|2. Preprocessing을 개선하고, fine-tuning을 통해 모델의 성능을 개선시켰다.|Validation accuracy를 90% 이상으로 개선하였다.|\n",
    "|3. 모델 학습에 Bucketing을 성공적으로 적용하고, 그 결과를 비교분석하였다.|Bucketing task을 수행하여 fine-tuning 시 연산 속도와 모델 성능 간의 trade-off 관계가 발생하는지 여부를 확인하고, 분석한 결과를 제시하였다.|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
