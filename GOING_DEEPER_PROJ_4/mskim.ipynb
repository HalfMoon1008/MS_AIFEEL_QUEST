{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd0ae870",
   "metadata": {},
   "source": [
    "# 12.Seq2seq으로 번역기 만들기[프로젝트]\n",
    "\n",
    "---\n",
    "\n",
    "## 12-1. 프로젝트: 한영 번역기 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dba0f5",
   "metadata": {},
   "source": [
    "### **라이브러리 버전을 확인해 봅니다**\n",
    "\n",
    "---\n",
    "\n",
    "사용할 라이브러리 버전을 둘러봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c8bdca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.3\n",
      "2.6.0\n",
      "3.4.3\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import tensorflow\n",
    "import matplotlib\n",
    "\n",
    "print(pandas.__version__)\n",
    "print(tensorflow.__version__)\n",
    "print(matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7019692",
   "metadata": {},
   "source": [
    "#### Step 1. 데이터 다운로드\n",
    "\n",
    "---\n",
    "\n",
    "아래 링크에서 `korean-english-park.train.tar.gz`를 다운로드받아 한영 병렬 데이터를 확보합니다.\n",
    "- [jungyeul/korean-parallel-corpora](https://github.com/jungyeul/korean-parallel-corpora/tree/master/korean-english-news-v1)\n",
    "\n",
    "---\n",
    "\n",
    "`~/aiffel/going_deeper_data/korean-english-park.train.tar.gz` 파일 구축 완료  \n",
    "\n",
    "- 압축 해제 완료\n",
    "    - `~/aiffel/going_deeper_data/korean-english-park.train.en`\n",
    "    - `~/aiffel/going_deeper_data/korean-english-park.train.ko`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a93e671",
   "metadata": {},
   "source": [
    "#### Step 2. 데이터 정제\n",
    "\n",
    "---\n",
    "\n",
    "1. `set` 데이터형이 **중복을 허용하지 않는다는 것을 활용**해 중복된 데이터를 제거하도록 합니다. 데이터의 **병렬 쌍이 흐트러지지 않게 주의**하세요! 중복을 제거한 데이터를 `cleaned_corpus`에 저장합니다.\n",
    "2. 앞서 정의한 `preprocessing()` 함수는 한글에서는 동작하지 않습니다. **한글에 적용할 수 있는 정규식**을 추가하여 함수를 재정의하세요!\n",
    "3. 타겟 언어인 영문엔 `<start>` 토큰과 `<end>`토큰을 추가하고 `split()`함수를 이용하여 토큰화합니다. 한글 토큰화는 KoNLPy의 `mecab` 클래스를 사용합니다.\n",
    "\n",
    "모든 데이터를 사용할 경우 학습에 굉장이 오랜 시간이 걸립니다. `cleaned_corpus`로부터 **토큰의 길이가 40 이하인 데이터를 선별**하여 `eng_corpus`와 `kor_corpus`를 각각 구축하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c636db61",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*데이터 확인(file read)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33227797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 94123\n",
      "Example:\n",
      ">> 개인용 컴퓨터 사용의 상당 부분은 \"이것보다 뛰어날 수 있느냐?\"\n",
      ">> 북한의 핵무기 계획을 포기하도록 하려는 압력이 거세지고 있는 가운데, 일본과 북한의 외교관들이 외교 관계를 정상화하려는 회담을 재개했다.\n",
      ">> \"경호 로보트가 침입자나 화재를 탐지하기 위해서 개인적으로, 그리고 전문적으로 사용되고 있습니다.\"\n",
      ">> 수자원부 당국은 논란이 되고 있고, 막대한 비용이 드는 이 사업에 대해 내년에 건설을 시작할 계획이다.\n",
      ">> 또한 근력 운동은 활발하게 걷는 것이나 최소한 20분 동안 뛰는 것과 같은 유산소 활동에서 얻는 운동 효과를 심장과 폐에 주지 않기 때문에, 연구학자들은 근력 운동이 심장에 큰 영향을 미치는지 여부에 대해 논쟁을 해왔다.\n"
     ]
    }
   ],
   "source": [
    "# 한글 데이터 읽기\n",
    "with open('/aiffel/aiffel/going_deeper_data/korean-english-park.train.ko', \"r\") as f:\n",
    "    ko_raw = f.read().splitlines()\n",
    "    \n",
    "print(\"Data Size:\", len(ko_raw))\n",
    "print(\"Example:\")\n",
    "\n",
    "for sen in ko_raw[0:100][::20]: print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26a14c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 94123\n",
      "Example:\n",
      ">> Much of personal computing is about \"can you top this?\"\n",
      ">> Amid mounting pressure on North Korea to abandon its nuclear weapons program Japanese and North Korean diplomats have resumed talks on normalizing diplomatic relations.\n",
      ">> “Guard robots are used privately and professionally to detect intruders or fire,” Karlsson said.\n",
      ">> Authorities from the Water Resources Ministry plan to begin construction next year on the controversial and hugely expensive project.\n",
      ">> Researchers also have debated whether weight-training has a big impact on the heart, since it does not give the heart and lungs the kind of workout they get from aerobic activities such as brisk walking or running for at least 20 minutes.\n"
     ]
    }
   ],
   "source": [
    "# 영어 데이터 읽기\n",
    "with open('/aiffel/aiffel/going_deeper_data/korean-english-park.train.en', \"r\") as f:\n",
    "    en_raw = f.read().splitlines()\n",
    "    \n",
    "print(\"Data Size:\", len(en_raw))\n",
    "print(\"Example:\")\n",
    "\n",
    "for sen in en_raw[0:100][::20]: print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f45d6c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*2. 앞서 정의한 `preprocessing()` 함수는 한글에서 동작하지 않습니다. **한글에 적용할 수 있는 정규식**을 추가하여 함수를 재정의하세요!*  \n",
    "  \n",
    "한글 문장 전처리 함수 &rarr; `preprocess_ko_sentence()`  \n",
    "영어 문장 전처리 함수 &rarr; `preprocess_en_sentence()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a7dbdf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess_ko_sentence() 선언 완료!\n"
     ]
    }
   ],
   "source": [
    "# 한글 문장 전처리 함수 선언\n",
    "def preprocess_ko_sentence(sentence, s_token=False, e_token=False):\n",
    "    sentence = sentence.lower().strip()\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣]+\", \" \", sentence)\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    if s_token:\n",
    "        sentence = '<start> ' + sentence\n",
    "\n",
    "    if e_token:\n",
    "        sentence += ' <end>'\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "print(\"preprocess_ko_sentence() 선언 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a5beac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess_en_sentence() 선언 완료\n"
     ]
    }
   ],
   "source": [
    "# 영어 문장 전처리 함수 선언\n",
    "def preprocess_en_sentence(sentence, s_token=False, e_token=False):\n",
    "    sentence = sentence.lower().strip()\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    if s_token:\n",
    "        sentence = '<start> ' + sentence\n",
    "\n",
    "    if e_token:\n",
    "        sentence += ' <end>'\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "print(\"preprocess_en_sentence() 선언 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e2375b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*1. `set` 데이터형이 **중복을 허용하지 않는다는 것을 활용**해 중복된 데이터를 제거하도록 합니다. 데이터의 **병렬 쌍이 흐트러지지 않게 주의**하세요! 중복을 제거한 데이터를 `cleaned_corpus`에 저장합니다. *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a29d160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "read languages raw..: 94123it [00:03, 24531.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed_ko 전체 길이 : 94123\n",
      "preprocessed_en 전체 길이 : 94123\n",
      "cleaned_corpus 전체 길이 : 78794\n",
      "('국제 유가와 금가격도 큰 폭 으로 떨어졌다', '<start> oil and gold prices plunged . <end>')\n",
      "('피트는 일 칸 국제영화제에서 처음으로 공개되는 이 영화의 프로듀서다', '<start> pitt is a producer on the movie , which had its world premiere at the cannes film festival on monday . <end>')\n",
      "('이 소식통은 오클리가 일 녹스빌 연방수사국 사무실에 자진 출두 혐의를 인정했다고 말했다', '<start> he voluntarily surrendered thursday at an fbi field office in knoxville , the sources said . <end>')\n",
      "('옥스포드 거리 상점은 모스의 특별 쇼핑 세션 참여로 변화를 맞았다', '<start> and so the masses were happy to wait hours in line outside topshop s oxford street store for the chance to attend a special shopping session of moss s inspirations . <end>')\n",
      "('머큐리 에너지 직원이 일 전기를 끊기 위해 집으로 방문하기 전까지 뮬리아가는 달러 약 만 원 를 연체했다', '<start> six days before a mercury energy representative arrived tuesday at the house to disconnect the electricity , she was in arrears . <end>')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# 위에서 선언한 함수 실행 후 저장될 배열 선언\n",
    "preprocessed_ko = []\n",
    "preprocessed_en = []\n",
    "\n",
    "# 한글 문장, 영어 문장 배열 동시에 읽어 들어가기\n",
    "for (v1, v2) in tqdm(zip(ko_raw, en_raw), desc='read languages raw..'):\n",
    "    # 한글 문장 전처리하면서 저장\n",
    "    preprocessed_ko.append(preprocess_ko_sentence(v1))\n",
    "    # 영어 문장 전처리하면서 저장\n",
    "    preprocessed_en.append(preprocess_en_sentence(v2, s_token=True, e_token=True))\n",
    "\n",
    "# set 타입 활용하여 중복된 데이터 제거 후 cleaned_corpus에 저장\n",
    "cleaned_corpus = set(zip(preprocessed_ko, preprocessed_en))\n",
    "# preprocessed_ko, preprocessed_en 길이들 확인\n",
    "print(f\"preprocessed_ko 전체 길이 : {len(preprocessed_ko)}\")\n",
    "print(f\"preprocessed_en 전체 길이 : {len(preprocessed_en)}\")\n",
    "# cleaned_corpus 전체 길이 확인\n",
    "print(f\"cleaned_corpus 전체 길이 : {len(cleaned_corpus)}\")\n",
    "# # 데이터 확인\n",
    "cnt = 0\n",
    "for val in cleaned_corpus:\n",
    "    print(val)\n",
    "    cnt += 1\n",
    "    \n",
    "    if cnt > 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f311178",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "3. 타겟 언어인 영문엔 `<start>` 토큰과 `<end>`토큰을 추가하고 `split()`함수를 이용하여 토큰화합니다. 한글 토큰화는 KoNLPy의 `mecab` 클래스를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1769880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이징 함수 - 한국어\n",
    "def tokenize_kr(sentence):\n",
    "    import MeCab\n",
    "\n",
    "    # MeCab 초기화\n",
    "    m = MeCab.Tagger()\n",
    "    \n",
    "    # 형태소, 품사 등이 있는 문장들로 반환\n",
    "    morphs = m.parse(sentence)\n",
    "    \n",
    "    # 형태소와 품사 정보를 출력\n",
    "    lines = morphs.split('\\n')\n",
    "    \n",
    "    # 특정 품사 단어들을 담을 토큰 리스트\n",
    "    tokens = []\n",
    "    \n",
    "    for line in lines:\n",
    "        if line == \"EOS\" or line == '':\n",
    "            break\n",
    "        else:\n",
    "            morph_info = line.split('\\t')\n",
    "            if len(morph_info) >= 2:\n",
    "                # 형태소\n",
    "                word = morph_info[0]\n",
    "                # 품사\n",
    "                pos = morph_info[1].split(',')[0]\n",
    "                # 품사가 명사 또는 고유명사인 경우 저장\n",
    "                if pos in [\"NNG\", \"NNP\"]:\n",
    "                    tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f904a459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이징 함수 - 영어\n",
    "def tokenize_en(sentence):\n",
    "    # 영문 - <start> & <end> 토큰 추가 --> 이미 처리되어 들어옴\n",
    "    # split()을 이용하여 토큰화\n",
    "    tokens = sentence.split(' ')\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b909c3a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "모든 데이터를 사용할 경우 학습에 굉장이 오랜 시간이 걸립니다. `cleaned_corpus`로부터 **토큰의 길이가 40 이하인 데이터를 선별**하여 `eng_corpus`와 `kor_corpus`를 각각 구축하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fc33b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "check kr & en corpus...: 100%|██████████| 78794/78794 [01:44<00:00, 755.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# 언어별 corpus\n",
    "eng_corpus = []\n",
    "kor_corpus = []\n",
    "\n",
    "# 토큰 길이 제한\n",
    "max_len = 40\n",
    "\n",
    "# for loop\n",
    "for val in tqdm(cleaned_corpus, desc='check kr & en corpus...'):\n",
    "    # 문장 당 토큰 담을 리스트\n",
    "    ko_sentence = []\n",
    "    en_sentence = []\n",
    "    \n",
    "    # 한국어 case\n",
    "    # 한국어 문장 하나 당 명사&고유명사 리스트 반환)\n",
    "    ko_sentence = tokenize_kr(val[0])\n",
    "    \n",
    "    # 영어 case\n",
    "    # 영어 문장 하나 당 단어 리스트 반환 (<start>, <end> 토큰 포함)\n",
    "    en_sentence = tokenize_en(val[1])\n",
    "    \n",
    "    if len(ko_sentence) <= max_len:\n",
    "        # 토큰 뭉치가 길이 40 이하일 때만 저장 (한국어 기준)\n",
    "        kor_corpus.append(ko_sentence)\n",
    "        eng_corpus.append(en_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6d1ea3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kor 길이 : 78785\n",
      "en 길이 : 78785\n"
     ]
    }
   ],
   "source": [
    "print(f\"kor 길이 : {len(kor_corpus)}\")\n",
    "print(f\"en 길이 : {len(eng_corpus)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec0ee6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['국제', '유가', '금', '가격', '폭']\n",
      "['<start>', 'oil', 'and', 'gold', 'prices', 'plunged', '.', '<end>']\n"
     ]
    }
   ],
   "source": [
    "print(kor_corpus[0])\n",
    "print(eng_corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac3e916",
   "metadata": {},
   "source": [
    "#### Step 3. 데이터 토큰화\n",
    "\n",
    "---\n",
    "\n",
    "앞서 정의한 `tokenize()` 함수를 사용해 데이터를 텐서로 변환하고 각각의 `tokenizer`를 얻으세요! 단어의 수는 실험을 통해 적당한 값을 맞춰주도록 합니다.! (최소 10,000 이상!)\n",
    "\n",
    "> *주의: 난이도에 비해 데이터가 많지 않아 훈련 데이터와 검증 데이터를 따로 나누지는 않습니다.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af64a617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize() 함수 선언\n",
    "def tokenize(corpus):\n",
    "    # 단어 빈도 수 상위 1만개\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=10000, filters='', oov_token=\"<unk>\")\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "    return tensor, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77afbbf",
   "metadata": {},
   "source": [
    "**강의처럼 3만개를 기준으로 하지 않고 전체 기준에서 데이터 unique만 신경 써서 7만개가 넘음**  \n",
    "그래서 `train`과 `test`로 나눠봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3973d704",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.text.Tokenizer object at 0x7fa35c806700>\n",
      "['국제', '유가', '금', '가격', '폭']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "# 토큰화하기\n",
    "enc_tensor, enc_tokenizer = tokenize(kor_corpus)\n",
    "dec_tensor, dec_tokenizer = tokenize(eng_corpus)\n",
    "# train_test_split을 활용해서 훈련 데이터와 검증 데이터로 분리하기\n",
    "enc_train, enc_test, dec_train, dec_test = train_test_split(enc_tensor, dec_tensor, test_size=0.2)\n",
    "\n",
    "print(enc_tokenizer)\n",
    "print(kor_corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a101b3f8",
   "metadata": {},
   "source": [
    "#### Step 4. 모델 설계\n",
    "\n",
    "---\n",
    "\n",
    "한국어를 영어로 잘 번역해 줄 멋진 **Attention 기반 Seq2seq 모델을 설계**하세요! 앞서 만든 모델에 `Dropout` 모듈을 추가하면 성능이 더 좋아집니다! `Embedding Size`와 `Hidden Size`는 실험을 통해 적당한 값을 맞춰 주도록 합니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8edab79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝~\n"
     ]
    }
   ],
   "source": [
    "# 강의 모델을 그대로 따라가보자\n",
    "## 1. Bahdanau Attention\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.w_dec = tf.keras.layers.Dense(units)\n",
    "        self.w_enc = tf.keras.layers.Dense(units)\n",
    "        self.w_com = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, h_enc, h_dec):\n",
    "        # h_enc shape: [batch x length x units]\n",
    "        # h_dec shape: [batch x units]\n",
    "\n",
    "        h_enc = self.w_enc(h_enc)\n",
    "        h_dec = tf.expand_dims(h_dec, 1)\n",
    "        h_dec = self.w_dec(h_dec)\n",
    "\n",
    "        score = self.w_com(tf.nn.tanh(h_dec + h_enc))\n",
    "        \n",
    "        attn = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vec = attn * h_enc\n",
    "        context_vec = tf.reduce_sum(context_vec, axis=1)\n",
    "\n",
    "        return context_vec, attn\n",
    "\n",
    "print(\"슝~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27cbfb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Encoder\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        # TODO: Awesome Encoder Modules\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(enc_units,\n",
    "                                       return_sequences=True)\n",
    "\t\t\n",
    "    def call(self, x):\n",
    "        # TODO: Awesome Process\n",
    "        out = self.embedding(x)\n",
    "        out = self.gru(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00f9f796",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Decoder\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        # TODO: Awesome Decoder Modules\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        self.attention = BahdanauAttention(self.dec_units)   # Attention 필수 사용!\n",
    "    \n",
    "    def call(self, x, h_dec, enc_out):\n",
    "        # TODO: Awesome Process\n",
    "        context_vec, attn = self.attention(enc_out, h_dec)\n",
    "\n",
    "        out = self.embedding(x)\n",
    "        out = tf.concat([tf.expand_dims(context_vec, 1), out], axis=-1)\n",
    "        \n",
    "        out, h_dec = self.gru(out)\n",
    "        out = tf.reshape(out, (-1, out.shape[2]))\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, h_dec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e773a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Output: (32, 30, 1024)\n",
      "Decoder Output: (32, 44565)\n",
      "Decoder Hidden State: (32, 1024)\n",
      "Attention: (32, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "# 코드를 실행하세요.\n",
    "## model.summary()\n",
    "### original BATCH_SIZE is 64\n",
    "BATCH_SIZE     = 32\n",
    "SRC_VOCAB_SIZE = len(enc_tokenizer.index_word) + 1\n",
    "TGT_VOCAB_SIZE = len(dec_tokenizer.index_word) + 1\n",
    "\n",
    "units         = 1024\n",
    "embedding_dim = 512\n",
    "\n",
    "encoder = Encoder(SRC_VOCAB_SIZE, embedding_dim, units)\n",
    "decoder = Decoder(TGT_VOCAB_SIZE, embedding_dim, units)\n",
    "\n",
    "# sample input\n",
    "sequence_len = 30\n",
    "\n",
    "sample_enc = tf.random.uniform((BATCH_SIZE, sequence_len))\n",
    "sample_output = encoder(sample_enc)\n",
    "\n",
    "print ('Encoder Output:', sample_output.shape)\n",
    "\n",
    "sample_state = tf.random.uniform((BATCH_SIZE, units))\n",
    "\n",
    "sample_logits, h_dec, attn = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                     sample_state, sample_output)\n",
    "\n",
    "print ('Decoder Output:', sample_logits.shape)\n",
    "print ('Decoder Hidden State:', h_dec.shape)\n",
    "print ('Attention:', attn.shape)\n",
    "\n",
    "del sample_output\n",
    "del sample_logits\n",
    "del h_dec\n",
    "del attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c1117a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝~\n"
     ]
    }
   ],
   "source": [
    "## optimizier & loss 구현\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "print(\"슝~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9eabe0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝~\n"
     ]
    }
   ],
   "source": [
    "## train step 구현하기\n",
    "@tf.function\n",
    "def train_step(src, tgt, encoder, decoder, optimizer, dec_tok):\n",
    "    bsz = src.shape[0]\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_out = encoder(src)\n",
    "        h_dec = enc_out[:, -1]\n",
    "        \n",
    "        dec_src = tf.expand_dims([dec_tok.word_index['<start>']] * bsz, 1)\n",
    "\n",
    "        for t in range(1, tgt.shape[1]):\n",
    "            pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n",
    "\n",
    "            loss += loss_function(tgt[:, t], pred)\n",
    "            dec_src = tf.expand_dims(tgt[:, t], 1)\n",
    "        \n",
    "    batch_loss = (loss / int(tgt.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return batch_loss\n",
    "\n",
    "print(\"슝~\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114506c1",
   "metadata": {},
   "source": [
    "강의 노드에서 이야기 하는  \n",
    "`train_step()`의 학습 과정\n",
    "\n",
    "---\n",
    "\n",
    "1. Encoder에 소스 문장을 전달해 **컨텍스트 벡터**인 `enc_out` 을 생성\n",
    "2. **t=0일 때,** Decoder의 Hidden State는 **Encoder의 Final State로 정의**. `h_dec = enc_out[:, -1]`\n",
    "3. Decoder에 입력으로 전달할 `<start>` **토큰 문장 생성**\n",
    "4. `<start>` 문장과 enc_out, Hidden State를 기반으로 다음 단어(t=1)를 예측. `pred`\n",
    "5. 예측된 단어와 정답 간의 Loss를 구한 후, t=1의 **정답 단어를 다음 입력으로 사용** (예측 단어 X)\n",
    "6. **반복!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787cf6ad",
   "metadata": {},
   "source": [
    "#### Step 5. 훈련하기\n",
    "\n",
    "---\n",
    "\n",
    "훈련엔 위에서 사용한 코드를 그대로 사용하되, `eval_step()` 부분이 없음에 유의합니다! 매 스탭 아래의 예문에 대한 번역을 생성하여 **본인이 생각하기에 가장 멋지게 번역한 Case를 제출**하세요! (Attention Map을 시각화해보는 것도 재밌을 거에요!)\n",
    "\n",
    "> 참고: 데이터의 난이도가 높은 편이므로 생각만큼 결과가 잘 안나올 수 있습니다.\n",
    "\n",
    "```shell\n",
    "## 예문 ##\n",
    "K1) 오바마는 대통령이다.\n",
    "K2) 시민들은 도시 속에 산다.\n",
    "K3) 커피는 필요 없다.\n",
    "K4) 일곱 명의 사망자가 발생했다.\n",
    "\n",
    "## 제출 ##\n",
    "E1) obama is the president . <end>\n",
    "E2) people are victims of the city . <end>\n",
    "E2) the price is not enough . <end>\n",
    "E2) seven people have died . <end>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7545515",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1970 [01:54<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "   OOM when allocating tensor with shape[1024,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node transpose_4}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[decoder_1/gru_3/PartitionedCall_48]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_step_364254]\n\nFunction call stack:\ntrain_step -> train_step -> train_step\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_167/3878301771.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         batch_loss = train_step(enc_train[idx:idx+BATCH_SIZE],\n\u001b[0m\u001b[1;32m     17\u001b[0m                                 \u001b[0mdec_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                                 \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1963\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m:    OOM when allocating tensor with shape[1024,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node transpose_4}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[decoder_1/gru_3/PartitionedCall_48]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_step_364254]\n\nFunction call stack:\ntrain_step -> train_step -> train_step\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습 #\n",
    "from tqdm import tqdm    # tqdm\n",
    "import random\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)    # tqdm\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss = train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                                dec_train[idx:idx+BATCH_SIZE],\n",
    "                                encoder,\n",
    "                                decoder,\n",
    "                                optimizer,\n",
    "                                dec_tokenizer)\n",
    "    \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))    # tqdm\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))    # tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca32a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, encoder, decoder):\n",
    "    attention = np.zeros((dec_train.shape[-1], enc_train.shape[-1]))\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = enc_tokenizer.texts_to_sequences([sentence.split()])\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    enc_out = encoder(inputs)\n",
    "\n",
    "    print(dec_tokenizer)\n",
    "\n",
    "    dec_hidden = enc_out[:, -1]\n",
    "    dec_input = tf.expand_dims([dec_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(dec_train.shape[-1]):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0]).numpy()\n",
    "\n",
    "        result += dec_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if dec_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention\n",
    "\n",
    "\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def translate(sentence, encoder, decoder):\n",
    "    result, sentence, attention = evaluate(sentence, encoder, decoder)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    attention = attention[:len(result.split()), :len(sentence.split())]\n",
    "    plot_attention(attention, sentence.split(), result.split(' '))\n",
    "\n",
    "\n",
    "translate(\"커피 좀 줄래요?\", encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af9567c",
   "metadata": {},
   "source": [
    "**루브릭**  \n",
    "아래의 기준을 바탕으로 프로젝트를 평가합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb064d5",
   "metadata": {},
   "source": [
    "|**평가문항**|**상세기준**|\n",
    "|---|---|\n",
    "|1. 번역기 모델 학습에 필요한 텍스트 데이터 전처리가 한국어 포함하여 잘 이루어졌다.|구두점, 대소문자, 띄어쓰기, 한글 형태소분석 등 번역기 모델에 요구되는 전처리가 정상적으로 진행되었다.|\n",
    "|2. Attentional Seq2seq 모델이 정상적으로 구동된다.|seq2seq2 모델 훈련 과정에서 training loss가 안정적으로 떨어지면서 학습이 진행됨이 확인되었다.|\n",
    "|3. 테스트 결과 의미가 통하는 수준의 번역문이 생성되었다.|테스트용 디코더 모델이 정상적으로 만들어져서, 정답과 어느 정도 유사한 영어 번역이 진행됨을 확인하였다.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f10734f",
   "metadata": {},
   "source": [
    "# 개인 회고\n",
    "\n",
    "---\n",
    "\n",
    "과정 확인을 해보면서 코드 수정해보고 싶었는데 OOM 에러가 훈련과정에서 떠서 아쉽 ㅠ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3cc6f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
